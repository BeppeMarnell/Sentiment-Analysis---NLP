{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "###Load vectors directly from the Google file \n",
    "embeddingWords = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###Import all classes needed\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import models\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###import the training data from file\n",
    "df_train = pd.read_csv('TrainingTwitterFinal20K.csv')\n",
    "df_train.dropna(axis=0, inplace=True)\n",
    "\n",
    "###Method to clean each sentence\n",
    "def clean(text):\n",
    "    if type(text) != str or text == '':\n",
    "        return ''\n",
    "\n",
    "    text = re.sub(\"\\\"\", \" \", text)\n",
    "    text = re.sub(\",\", \" \", text)\n",
    "    \n",
    "    text = re.sub(\"  \", \" \", text)\n",
    "    text = re.sub(\"-\", \" \", text)\n",
    "    \n",
    "    text = re.sub(\"\\'s\", \" is\", text)\n",
    "    text = re.sub(\" whats \", \"what is\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(\"can\\'t\", \"cannot\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"don\\'t\", \"do not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"what\\'s\", \"what is\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"aren\\'t\", \"are not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"it\\'s\", \"it is\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"dont\", \"do not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"how\\'s'\", \"how is\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"isn\\'t\", \"is not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"jrk\", \"jerk\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"shoulda\", \"should have\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'m\", \" am\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    text = re.sub(r\"@\\S+\", '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\W+', ' ', text, flags=re.IGNORECASE)\n",
    "    \n",
    "    ###remove stopwords\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    text_file = open(\"list.txt\", \"r\")\n",
    "    lines = text_file.readlines()\n",
    "    lines = re.sub('\"', '', lines[0])\n",
    "    lines = re.sub(' ', '', lines)\n",
    "    lines = re.sub('\\n', '', lines)\n",
    "    stopWords2 = lines.split(',')\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    wordsFiltered = []\n",
    "    \n",
    "    '''\n",
    "    # lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            if w not in stopWords2:\n",
    "                wordsFiltered.append(lemmatizer.lemmatize(w))\n",
    "    '''\n",
    "    ###stemmer\n",
    "    stemmer = EnglishStemmer()\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            if w not in stopWords2:\n",
    "                wordsFiltered.append(stemmer.stem(w))\n",
    "    \n",
    "    ###Return a list of words\n",
    "    return sorted(list(set(wordsFiltered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###Apply clean to the train data\n",
    "df_train['comment'] = df_train['comment'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###sum the vectors given a certain size\n",
    "def sumvector(vec1, vec2 ,size):\n",
    "    if vec1 is None:\n",
    "        vec1 = [0] * size\n",
    "    if vec2 is None:\n",
    "        vec2 = [0] * size\n",
    "\n",
    "    vecsum = []\n",
    "    for i in range(len(vec1)):\n",
    "        vecsum.append(vec1[i] + vec2[i])\n",
    "\n",
    "    return vecsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###Get values from the train data\n",
    "c1 = df_train['comment'].values.tolist()\n",
    "\n",
    "###Create a model using word2vec algorithm using the train data\n",
    "model = models.Word2Vec(c1, min_count=1, size=128)\n",
    "\n",
    "###Create the vectors list for all the sentences\n",
    "C_vectors = [None] * 21704\n",
    "sizeVect= 10\n",
    "\n",
    "for i, sentence in enumerate(df_train['comment']):\n",
    "\n",
    "    for j, sentence2 in enumerate(df_train['comment'][i]):\n",
    "        \n",
    "        if df_train['comment'][i][j] in embeddingWords.vocab:\n",
    "            C_vectors[i] = sumvector(C_vectors[i], embeddingWords[df_train['comment'][i][j]], sizeVect)\n",
    "        else:\n",
    "            C_vectors[i] = sumvector(C_vectors[i], model.wv[df_train['comment'][i][j]], sizeVect)\n",
    "    \n",
    "    if C_vectors[i] is None:\n",
    "        C_vectors[i] = [0] * 10\n",
    "\n",
    "\n",
    "###Create a classifier Support vector machines\n",
    "#SvmClass = svm.SVC(gamma=\"scale\", C = 1.6) #50k\n",
    "SvmClass = svm.SVC(gamma=\"scale\", C = 2.00) #20k\n",
    "#SvmClass = svm.SVC(gamma=\"scale\", C = 1.40) #100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "###Label the training set data\n",
    "labels = (df_train['positivity'].values.tolist())\n",
    "\n",
    "###Fit the classifier\n",
    "SvmClass.fit(C_vectors, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Prepare test set to plug it in to the classifier\n",
    "df_test = pd.read_csv('400RC.csv', error_bad_lines=False)\n",
    "\n",
    "df_test['comment'] = df_test['comment'].apply(clean)\n",
    "\n",
    "###Get values from the test data\n",
    "T1 = df_test['comment'].values.tolist()\n",
    "#create a model using word2vec algorithm using the train data\n",
    "modelTest = models.Word2Vec(T1, min_count=1, size=10)\n",
    "\n",
    "###Compute labels for the test data\n",
    "labels = df_test['positivity'].values\n",
    "\n",
    "###Create the vectors of each sentences\n",
    "C_vectors2 = [None] * 400\n",
    "\n",
    "for i, sentence in enumerate(df_test['comment']):\n",
    "    for j, sentence2 in enumerate(df_test['comment'][i]):\n",
    "        \n",
    "        if df_test['comment'][i][j] in embeddingWords.vocab:\n",
    "            C_vectors2[i] = sumvector(C_vectors2[i], embeddingWords[df_test['comment'][i][j]], sizeVect)\n",
    "        else:\n",
    "            C_vectors2[i] = sumvector(C_vectors2[i], modelTest.wv[df_test['comment'][i][j]], sizeVect)\n",
    "    \n",
    "    if C_vectors[i] is None:\n",
    "        C_vectors[i] = [0] * 10\n",
    "\n",
    "###Calculate accuracy of the classifier\n",
    "predicted = SvmClass.predict(C_vectors2)\n",
    "\n",
    "print(np.mean(predicted == labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "###Method to print a complete confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "    ###Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    classes = ('Negative', 'Positive')\n",
    "    \n",
    "    ###We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    ###Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    ###Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "###Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(labels, predicted, title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
