{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "###Load vectors directly from the Google file \n",
    "embeddingWords = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###Import all classes needed\n",
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import models\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "###Import the training data from file\n",
    "df_train = pd.read_csv('TrainingTwitterFinal20K.csv')\n",
    "df_train.dropna(axis=0, inplace=True)\n",
    "\n",
    "###Method to clean each sentence\n",
    "def clean(text):\n",
    "    \n",
    "    if type(text) != str or text == '':\n",
    "        return ''\n",
    "     \n",
    "    text = re.sub(\"\\'s\", \"is\", text)\n",
    "    text = re.sub(\" whats \", \"what is\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(\"can\\'t\", \"cannot\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"don\\'t\", \"do not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"what\\'s\", \"what is\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"aren\\'t\", \"are not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"it\\'s\", \"it is\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"dont\", \"do not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"how\\'s'\", \"how is\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"isn\\'t\", \"is not\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'re\", \" are\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"\\'m\", \" am\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"jrk\", \"jerk\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(\"shoulda\", \"should have\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\W+', ' URL ', text)\n",
    "    \n",
    "    ###remove comma between numbers, i.e. 15,000 -> 15000\n",
    "    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n",
    "    \n",
    "    ###remove stopwords\n",
    "    text_file = open(\"list.txt\", \"r\")\n",
    "    lines = text_file.readlines()\n",
    "    lines = re.sub('\"', '', lines[0])\n",
    "    lines = re.sub(' ', '', lines)\n",
    "    lines = re.sub('\\n', '', lines)\n",
    "    \n",
    "    stopWords2 = lines.split(',')\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    \n",
    "    ###stemmer\n",
    "    stemmer = EnglishStemmer()\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    wordsFiltered = []\n",
    "      \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "  \n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            if w not in stopWords2:        \n",
    "                wordsFiltered.append(lemmatizer.lemmatize(w))\n",
    "        \n",
    "\n",
    "    ###Return a list of words\n",
    "    return sorted(list(set(wordsFiltered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###Apply clean to the train data\n",
    "df_train['comment'] = df_train['comment'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###sum the vectors given a certain size\n",
    "def sumvector(vec1, vec2 ,size):\n",
    "    \n",
    "    if vec1 is None:\n",
    "        vec1 = [0] * size\n",
    "    \n",
    "    if vec2 is None:\n",
    "        vec2 = [0] * size\n",
    "\n",
    "    vecsum = []\n",
    "   \n",
    "    for i in range(len(vec1)):\n",
    "        vecsum.append(vec1[i] + vec2[i])\n",
    "\n",
    "    return vecsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "metadata": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "###create the vectors of each sentence with vectorizer\n",
    "allSentences = []\n",
    "\n",
    "for i, sentence in enumerate(df_train['comment']):\n",
    "    string = ' '.join(sentence)\n",
    "    allSentences.append(string)\n",
    "    \n",
    "vectorizer = CountVectorizer()\n",
    "sentenceVectors = vectorizer.fit_transform(allSentences)\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "###label the training set data\n",
    "labels = df_train['positivity'].values.tolist()\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "train = tfidf.fit_transform(sentenceVectors)\n",
    "train.shape\n",
    "\n",
    "###create a Multinomial Naive Bayes Classifier, input array for X values and labels s\n",
    "clf = MultinomialNB().fit(train, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "metadata": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Retrieving the test data\n",
    "df_test = pd.read_csv('2004RCnew.csv')\n",
    "\n",
    "###Apply clean\n",
    "df_test['comment'] = df_test['comment'].apply(clean)\n",
    "\n",
    "###Compute labels for the test data\n",
    "labels = df_test['positivity'].values\n",
    " \n",
    "allSentences2 = []\n",
    "\n",
    "for i, sentence in enumerate(df_test['comment']):\n",
    "    allSentences2.append(' '.join(sentence))\n",
    "    \n",
    "sentenceVectors2 = vectorizer.transform(allSentences2)\n",
    "\n",
    "###calculate accuracy of the classifier\n",
    "predicted = clf.predict(sentenceVectors2)\n",
    "\n",
    "print(np.mean(predicted == labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###Method to print a complete confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "    ###Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    classes = ('Negative', 'Positive')\n",
    "    \n",
    "    ###We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    ###Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    ###Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {}
   },
   "outputs": [],
   "source": [
    "###Plot non-normalized confusion matrix\n",
    "plot_confusion_matrix(labels, predicted, title='Confusion matrix, without normalization')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "stem_cell": {
   "cell_type": "raw",
   "metadata": {
    "pycharm": {
     "metadata": false
    }
   },
   "source": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
